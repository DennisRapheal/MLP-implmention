{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "(學習歷程)CNN卷積神經網路.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPEI8vQ6avQOqdtp9xcVugS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DennisRapheal/MLP-implmention/blob/main/(%E5%AD%B8%E7%BF%92%E6%AD%B7%E7%A8%8B)CNN%E5%8D%B7%E7%A9%8D%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eM1JTTSQsXM2"
      },
      "source": [
        "## **下載 Kaggle Cats and Dogs Dataset 資料集**\n",
        "\n",
        "Kaggle Cats and Dogs Dataset1l 包含了貓和狗的圖片各 15000 張\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "*   Step1~下載資料集\n",
        ">下載連結：https://www.microsoft.com/en-us/download/details.aspx?id=54765\n",
        ">\n",
        "> 解壓縮後會建立 < Petlmages > 目錄，包含 < Cats > 和 < Dogs >\n",
        "兩個目錄，\n",
        "*  Step2~讀取資料集\n",
        ">因為影像原始大小不一，先呼叫修改成一樣的尺寸。\n",
        ">\n",
        ">注意：在 < Cats > 和 < Dogs > 的目錄中各有一張0kb的假照片和一個 < Thumbs.db >檔，可以先手動把這些圖刪掉，避免讀取這些不合法的檔案，有可以用try~except \n",
        "\n",
        "* 讀取資料集的方法：\n",
        ">\n",
        "># python os ：\n",
        ">\n",
        ">\b os 模組是關於作業系統操作呼叫的相關模組\n",
        ">\n",
        ">#常見語法：https://www.itread01.com/content/1542016996.html\n",
        ">\n",
        ">\n",
        "># python OpenCV ：\n",
        ">\n",
        ">\b 用於讀取圖片或顯示圖片\n",
        ">\n",
        ">#常見語法：https://blog.gtwang.org/programming/opencv-basic-image-read-and-write-tutorial/\n",
        ">\n",
        ">\n",
        "># python glob：\n",
        ">\n",
        ">#常見語法：\n",
        ">\n",
        ">1.glob.glob（pathname), 返回所有匹配的文件路徑列表。它只有一個參數pathname，定義了文件路徑匹配規則，這裏可以是絕對路徑，也可以是相對路徑。\n",
        ">\n",
        ">2.glob.iglob(pathname), 獲取一個可編歷對象，使用它可以逐個獲取匹配的文件路徑名。與glob.glob()的區別是：glob.glob同時獲取所有的匹配路徑，而glob.iglob一次只獲取一個匹配路徑。\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "\n",
        "#將原始圖片resize後存在images串列，而標籤存在labels串列\n",
        "images=[]\n",
        "labels=[]\n",
        "dict_labels = {\"Cat\":0, \"Dog\":1}\n",
        "size=(40,40) #統一資料\n",
        "\n",
        "for folders in glob.glob(\"PetImages/*\"):\n",
        "  print(folders, \"圖片讀取中...\")\n",
        "  #只讀取貓夠的圖片\n",
        "  for filename in os.listdir(folders):\n",
        "    label=folders.split(\"\\\\\")[-1]\n",
        "    try:\n",
        "      img=cv2.imread(os.path.join(folders,filename))\n",
        "      if img is not None:\n",
        "        img=cv2.resize(img,dsize=size)\n",
        "        images.append(img)\n",
        "        labels.append(dict_labels[label])\n",
        "    except:\n",
        "      print(os.path.join(folders,filename),\"無法讀取！\")\n",
        "      pass\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "LbHnvp8u3Lk8",
        "outputId": "e9062795-1525-47bc-8b6e-138a81da30f2"
      },
      "source": [
        "from google.colab import files\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils\n",
        "np.random.seed(10)\n",
        "from keras.models import Sequential\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Dense\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "\n",
        "#將原始圖片resize後存在images串列，而標籤存在labels串列\n",
        "images=[]\n",
        "labels=[]\n",
        "dict_labels = {\"Cat\":0, \"Dog\":1}\n",
        "size=(40,40) #統一資料\n",
        "\n",
        "uploaded = files.upload()\n",
        "for folders in glob.glob(\"uploaded/*\"):\n",
        "  print(folders, \"圖片讀取中...\")\n",
        "  #只讀取貓夠的圖片\n",
        " \n",
        "  for filename in os.listdir(folders):\n",
        "      label=folders.split(\"\\\\\")[-1]\n",
        "      try:\n",
        "          img=cv2.imread(os.path.join(folders,filename))\n",
        "          if img is not None:\n",
        "             img=cv2.resize(img,dsize=size)\n",
        "             images.append(img)\n",
        "             labels.append(dict_labels[label])\n",
        "      except:\n",
        "            print(os.path.join(folders,filename),\"無法讀取！\")\n",
        "            pass\n",
        "\n",
        "print(\"下載完成\")\n",
        "print(len(images),len(labels))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ea4e3c91-a493-4cba-8a7d-53c77ed6d064\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ea4e3c91-a493-4cba-8a7d-53c77ed6d064\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving 0.jpg to 0.jpg\n",
            "下載完成\n",
            "0 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmICFS6ep-RA"
      },
      "source": [
        "#from google.colab import files\n",
        "#uploaded = files.upload()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfQbeVZNtZKV"
      },
      "source": [
        "# ***CIFAR-10資料集***\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7uTLMgZuw6k4",
        "outputId": "4362f65e-afcf-496e-d982-fce0cee03d8c"
      },
      "source": [
        "from keras.datasets import cifar10\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#載入資料\n",
        "((train_data, train_label),(test_data, test_label)) = cifar10.load_data()\n",
        "\n",
        "#資料標準化（0~1之間）\n",
        "train_data = train_data/255\n",
        "test_data = test_data/255\n",
        "\n",
        "#轉為 One_Hot Encoding\n",
        "#只看和真實質相同的label，也就是把和真實值相同的label設為\"1\"，其他設為\"0\"\n",
        "#先 from keras.utils import np_utils 再用np_utils.to_categorical()\n",
        "train_label_onehot = np_utils.to_categorical(train_label)\n",
        "test_label_onehot = np_utils.to_categorical(test_label)\n",
        "\n",
        "dropvalue = 0.4\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(input_shape=(32, 32, 3), filters=32, kernel_size=(5,5), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(dropvalue))\n",
        "\n",
        "model.add(Conv2D(filters=64, kernel_size=(5,5), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(dropvalue))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "# 以「.add」函式增加輸入層以及隱藏層\n",
        "# 「Dence(參數1,參數2,參數3,參數4)」為上下緊密連接的神經網路層\n",
        "# 參數1「unit＝256」：代表隱藏層神經元數目有256個\n",
        "# 參數2「input_dim=784」：代表輸入層神經元數目有784個\n",
        "# 參數3「kernal_initializer='normal'」：代表使用常態分佈的亂數初始化權重（weight）和偏置（bias）\n",
        "# 參數4「activation='relu'」：制定激勵函式，'relu':大於零為identity function，小於 0 輸出 0\n",
        "model.add(Dense(units=256, kernel_initializer='normal', activation='relu'))\n",
        "model.add(Dropout(dropvalue))\n",
        "\n",
        "model.add(Dense(units=10, kernel_initializer='normal', activation='softmax'))\n",
        "\n",
        "#訓練\n",
        "# 以「.compile(參數1,參數2,參數3)」來定義以下訓練模型\n",
        "# 參數1「loss='categorical_crossentropy'」：定義Loss function，設為categorical_crossentropy\n",
        "# 參數2「optimizer='adam'」：定義Optimizer（最佳化方法）為\"adam\"\n",
        "# 參數3「metrics=['accuracy']」：定義metrics（評估準確率的方法）為\"accuracy\"準確率\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "#model.fit(x=特徵法 ,y=標籤 ,validation_split = 驗證資料百分比 ,epochs=訓練資料 ,batch_size=每批次有多少筆)\n",
        "# x,y：設定訓練特徵值和標籤，兩個參數都是必須的\n",
        "# validation_split：例如0.2表示將訓練資料保留20%當作驗證資料。若省略則全部保留\n",
        "# epochs：訓練次數。若省略則訓練一次\n",
        "# verbose：設定是否顯示訓練過程（0不顯示、1詳細顯示、2簡易顯示）\n",
        "model.fit(x=train_data, y=train_label_onehot, validation_split=0.2, epochs=50, batch_size=500, verbose=2)\n",
        "\n",
        "#用測試資料評估準確率\n",
        "evaluate = model.evaluate(test_data, test_label_onehot)\n",
        "print('測試資料準確率：', evaluate[1])\n",
        "\n",
        "#模型儲存\n",
        "#model.save()\n",
        "\n",
        "#數據呈現\n",
        "# Data for plotting\n",
        "fig, ax = plt.subplots()\n",
        "# 對 figure 的 figsize 參數設定為 30×30 大小\n",
        "fig = plt.rc('figure', figsize=(30, 30))\n",
        "ax.plot(val_accuracy, 'b:', label='Predict') #準確率\n",
        "#ax.plot(test_y, 'r-', label='True')#美金匯率\n",
        "ax.set(xlabel='訓練次數', ylabel='Rate',\n",
        "       title='Accuracy')\n",
        "ax.legend(['預測', '美金匯率'])\n",
        "ax.grid()#增加隔線\n",
        "#fig.savefig(\"test.png\") #存取圖片\n",
        "plt.show()\n",
        "\n",
        "                \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "170508288/170498071 [==============================] - 2s 0us/step\n",
            "Epoch 1/50\n",
            "80/80 - 45s - loss: 1.9194 - accuracy: 0.2951 - val_loss: 1.6167 - val_accuracy: 0.4280\n",
            "Epoch 2/50\n",
            "80/80 - 1s - loss: 1.5640 - accuracy: 0.4306 - val_loss: 1.4276 - val_accuracy: 0.4943\n",
            "Epoch 3/50\n",
            "80/80 - 1s - loss: 1.4209 - accuracy: 0.4873 - val_loss: 1.2903 - val_accuracy: 0.5502\n",
            "Epoch 4/50\n",
            "80/80 - 1s - loss: 1.3188 - accuracy: 0.5261 - val_loss: 1.1937 - val_accuracy: 0.5820\n",
            "Epoch 5/50\n",
            "80/80 - 1s - loss: 1.2383 - accuracy: 0.5578 - val_loss: 1.1341 - val_accuracy: 0.6055\n",
            "Epoch 6/50\n",
            "80/80 - 1s - loss: 1.1761 - accuracy: 0.5774 - val_loss: 1.0830 - val_accuracy: 0.6233\n",
            "Epoch 7/50\n",
            "80/80 - 1s - loss: 1.1155 - accuracy: 0.6032 - val_loss: 1.0358 - val_accuracy: 0.6425\n",
            "Epoch 8/50\n",
            "80/80 - 1s - loss: 1.0780 - accuracy: 0.6162 - val_loss: 0.9907 - val_accuracy: 0.6620\n",
            "Epoch 9/50\n",
            "80/80 - 1s - loss: 1.0314 - accuracy: 0.6338 - val_loss: 0.9720 - val_accuracy: 0.6624\n",
            "Epoch 10/50\n",
            "80/80 - 1s - loss: 0.9977 - accuracy: 0.6442 - val_loss: 0.9253 - val_accuracy: 0.6834\n",
            "Epoch 11/50\n",
            "80/80 - 1s - loss: 0.9702 - accuracy: 0.6563 - val_loss: 0.9051 - val_accuracy: 0.6867\n",
            "Epoch 12/50\n",
            "80/80 - 1s - loss: 0.9357 - accuracy: 0.6685 - val_loss: 0.8883 - val_accuracy: 0.6906\n",
            "Epoch 13/50\n",
            "80/80 - 1s - loss: 0.9103 - accuracy: 0.6807 - val_loss: 0.8583 - val_accuracy: 0.7066\n",
            "Epoch 14/50\n",
            "80/80 - 1s - loss: 0.8912 - accuracy: 0.6857 - val_loss: 0.8327 - val_accuracy: 0.7109\n",
            "Epoch 15/50\n",
            "80/80 - 1s - loss: 0.8684 - accuracy: 0.6923 - val_loss: 0.8189 - val_accuracy: 0.7145\n",
            "Epoch 16/50\n",
            "80/80 - 1s - loss: 0.8443 - accuracy: 0.7006 - val_loss: 0.8372 - val_accuracy: 0.7086\n",
            "Epoch 17/50\n",
            "80/80 - 1s - loss: 0.8286 - accuracy: 0.7063 - val_loss: 0.8135 - val_accuracy: 0.7178\n",
            "Epoch 18/50\n",
            "80/80 - 1s - loss: 0.8014 - accuracy: 0.7189 - val_loss: 0.8028 - val_accuracy: 0.7233\n",
            "Epoch 19/50\n",
            "80/80 - 1s - loss: 0.7810 - accuracy: 0.7216 - val_loss: 0.7697 - val_accuracy: 0.7340\n",
            "Epoch 20/50\n",
            "80/80 - 1s - loss: 0.7729 - accuracy: 0.7265 - val_loss: 0.7637 - val_accuracy: 0.7372\n",
            "Epoch 21/50\n",
            "80/80 - 1s - loss: 0.7608 - accuracy: 0.7305 - val_loss: 0.7664 - val_accuracy: 0.7349\n",
            "Epoch 22/50\n",
            "80/80 - 1s - loss: 0.7339 - accuracy: 0.7398 - val_loss: 0.7654 - val_accuracy: 0.7347\n",
            "Epoch 23/50\n",
            "80/80 - 1s - loss: 0.7127 - accuracy: 0.7452 - val_loss: 0.7650 - val_accuracy: 0.7327\n",
            "Epoch 24/50\n",
            "80/80 - 1s - loss: 0.7134 - accuracy: 0.7473 - val_loss: 0.7594 - val_accuracy: 0.7374\n",
            "Epoch 25/50\n",
            "80/80 - 1s - loss: 0.6959 - accuracy: 0.7529 - val_loss: 0.7327 - val_accuracy: 0.7472\n",
            "Epoch 26/50\n",
            "80/80 - 1s - loss: 0.6771 - accuracy: 0.7607 - val_loss: 0.7266 - val_accuracy: 0.7502\n",
            "Epoch 27/50\n",
            "80/80 - 1s - loss: 0.6591 - accuracy: 0.7649 - val_loss: 0.7339 - val_accuracy: 0.7454\n",
            "Epoch 28/50\n",
            "80/80 - 1s - loss: 0.6506 - accuracy: 0.7687 - val_loss: 0.7168 - val_accuracy: 0.7546\n",
            "Epoch 29/50\n",
            "80/80 - 1s - loss: 0.6445 - accuracy: 0.7704 - val_loss: 0.7128 - val_accuracy: 0.7559\n",
            "Epoch 30/50\n",
            "80/80 - 1s - loss: 0.6259 - accuracy: 0.7772 - val_loss: 0.7056 - val_accuracy: 0.7558\n",
            "Epoch 31/50\n",
            "80/80 - 1s - loss: 0.6229 - accuracy: 0.7793 - val_loss: 0.7127 - val_accuracy: 0.7520\n",
            "Epoch 32/50\n",
            "80/80 - 1s - loss: 0.6161 - accuracy: 0.7809 - val_loss: 0.7372 - val_accuracy: 0.7460\n",
            "Epoch 33/50\n",
            "80/80 - 1s - loss: 0.5945 - accuracy: 0.7882 - val_loss: 0.7146 - val_accuracy: 0.7515\n",
            "Epoch 34/50\n",
            "80/80 - 1s - loss: 0.5876 - accuracy: 0.7907 - val_loss: 0.7213 - val_accuracy: 0.7516\n",
            "Epoch 35/50\n",
            "80/80 - 1s - loss: 0.5784 - accuracy: 0.7944 - val_loss: 0.7154 - val_accuracy: 0.7548\n",
            "Epoch 36/50\n",
            "80/80 - 1s - loss: 0.5678 - accuracy: 0.7985 - val_loss: 0.7119 - val_accuracy: 0.7564\n",
            "Epoch 37/50\n",
            "80/80 - 1s - loss: 0.5641 - accuracy: 0.7983 - val_loss: 0.7104 - val_accuracy: 0.7575\n",
            "Epoch 38/50\n",
            "80/80 - 1s - loss: 0.5497 - accuracy: 0.8019 - val_loss: 0.7189 - val_accuracy: 0.7568\n",
            "Epoch 39/50\n",
            "80/80 - 1s - loss: 0.5414 - accuracy: 0.8073 - val_loss: 0.6979 - val_accuracy: 0.7604\n",
            "Epoch 40/50\n",
            "80/80 - 1s - loss: 0.5282 - accuracy: 0.8115 - val_loss: 0.6984 - val_accuracy: 0.7597\n",
            "Epoch 41/50\n",
            "80/80 - 1s - loss: 0.5187 - accuracy: 0.8161 - val_loss: 0.7157 - val_accuracy: 0.7592\n",
            "Epoch 42/50\n",
            "80/80 - 1s - loss: 0.5164 - accuracy: 0.8158 - val_loss: 0.6887 - val_accuracy: 0.7656\n",
            "Epoch 43/50\n",
            "80/80 - 1s - loss: 0.5083 - accuracy: 0.8180 - val_loss: 0.6943 - val_accuracy: 0.7654\n",
            "Epoch 44/50\n",
            "80/80 - 1s - loss: 0.5017 - accuracy: 0.8204 - val_loss: 0.6892 - val_accuracy: 0.7661\n",
            "Epoch 45/50\n",
            "80/80 - 1s - loss: 0.4984 - accuracy: 0.8216 - val_loss: 0.6936 - val_accuracy: 0.7663\n",
            "Epoch 46/50\n",
            "80/80 - 1s - loss: 0.4896 - accuracy: 0.8233 - val_loss: 0.6987 - val_accuracy: 0.7652\n",
            "Epoch 47/50\n",
            "80/80 - 1s - loss: 0.4815 - accuracy: 0.8278 - val_loss: 0.7249 - val_accuracy: 0.7557\n",
            "Epoch 48/50\n",
            "80/80 - 1s - loss: 0.4831 - accuracy: 0.8282 - val_loss: 0.6916 - val_accuracy: 0.7683\n",
            "Epoch 49/50\n",
            "80/80 - 1s - loss: 0.4769 - accuracy: 0.8296 - val_loss: 0.6969 - val_accuracy: 0.7636\n",
            "Epoch 50/50\n",
            "80/80 - 1s - loss: 0.4709 - accuracy: 0.8317 - val_loss: 0.6952 - val_accuracy: 0.7675\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.7143 - accuracy: 0.7648\n",
            "測試資料準確率： 0.7648000121116638\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0cdb9d7e01ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m#數據呈現\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m# Data for plotting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;31m# 對 figure 的 figsize 參數設定為 30×30 大小\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'figure'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_Ogg0W-ZWlu"
      },
      "source": [
        "# **實作**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZwwcWPSZd4L"
      },
      "source": [
        "from keras.datasets import cifar10\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "#載入資料\n",
        "((train_data, train_label),(test_data, test_label)) = cifar10.load_data()\n",
        "\n",
        "print(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}